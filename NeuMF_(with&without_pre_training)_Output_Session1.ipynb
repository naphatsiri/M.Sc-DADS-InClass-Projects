{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNixjHbGP4lj9/LdVBojvsN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naphatsiri/DADS-InClass-Master-Degree/blob/main/NeuMF_(with%26without_pre_training)_Output_Session1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries if not already installed\n",
        "!pip install tensorflow keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYx9IVzCffMh",
        "outputId": "ec6a7e62-e9af-437a-b313-f83ca176dca8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCv7tCdilz4G",
        "outputId": "494e3878-09a1-40c2-97a7-45aaf9a6ffad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.12.1)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "-RGh9DDydVc4",
        "outputId": "79dfa8db-df0c-4fd4-8428-bfa5972a9925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating initial model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "evaluate_model() got an unexpected keyword argument 'topK'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-da95576a33c5>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Initial performance evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating initial model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mhits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestRatings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestNegatives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mhr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndcgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Init: HR = {hr:.4f}, NDCG = {ndcg:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: evaluate_model() got an unexpected keyword argument 'topK'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Input, Dense, Flatten, Concatenate, Multiply\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import time\n",
        "from evaluate import evaluate_model  # Ensure evaluate_model.py is uploaded in the Colab environment\n",
        "from Dataset import Dataset  # Ensure Dataset.py is uploaded in the Colab environment\n",
        "\n",
        "#################### Arguments ####################\n",
        "class Args:\n",
        "    path = './Data/'\n",
        "    dataset = 'ml-1m'\n",
        "    epochs = 100\n",
        "    batch_size = 256\n",
        "    num_factors = 8\n",
        "    layers = '[64,32,16,8]'\n",
        "    reg_mf = 0\n",
        "    reg_layers = '[0,0,0,0]'\n",
        "    num_neg = 4\n",
        "    lr = 0.001\n",
        "    verbose = 1\n",
        "    out = 1\n",
        "    mf_pretrain = 'ml-1m_GMF_model.h5'\n",
        "    mlp_pretrain = 'ml-1m_MLP_model.h5'\n",
        "\n",
        "args = Args()\n",
        "\n",
        "#################### Model Definition ####################\n",
        "def get_model(num_users, num_items, mf_dim=8, layers=[64, 32, 16, 8], reg_layers=[0, 0, 0, 0], reg_mf=0):\n",
        "    # Input layers\n",
        "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
        "    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
        "\n",
        "    # Embedding layers\n",
        "    MF_Embedding_User = Embedding(input_dim=num_users, output_dim=mf_dim, embeddings_regularizer=l2(reg_mf), input_length=1)\n",
        "    MF_Embedding_Item = Embedding(input_dim=num_items, output_dim=mf_dim, embeddings_regularizer=l2(reg_mf), input_length=1)\n",
        "\n",
        "    MLP_Embedding_User = Embedding(input_dim=num_users, output_dim=layers[0] // 2, embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
        "    MLP_Embedding_Item = Embedding(input_dim=num_items, output_dim=layers[0] // 2, embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
        "\n",
        "    # MF part\n",
        "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
        "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
        "    mf_vector = Multiply()([mf_user_latent, mf_item_latent])\n",
        "\n",
        "    # MLP part\n",
        "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
        "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
        "    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
        "    for idx in range(1, len(layers)):\n",
        "        mlp_vector = Dense(layers[idx], activation='relu', kernel_regularizer=l2(reg_layers[idx]))(mlp_vector)\n",
        "\n",
        "    # Concatenate MF and MLP parts\n",
        "    predict_vector = Concatenate()([mf_vector, mlp_vector])\n",
        "\n",
        "    # Prediction layer\n",
        "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name=\"prediction\")(predict_vector)\n",
        "\n",
        "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
        "    return model\n",
        "\n",
        "#################### Training & Evaluation ####################\n",
        "if __name__ == '__main__':\n",
        "    # Load Dataset\n",
        "    dataset = Dataset(args.path + args.dataset)\n",
        "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
        "    num_users, num_items = train.shape\n",
        "\n",
        "    # Load pre-trained models if available\n",
        "    gmf_model = tf.keras.models.load_model(args.mf_pretrain) if os.path.exists(args.mf_pretrain) else None\n",
        "    mlp_model = tf.keras.models.load_model(args.mlp_pretrain) if os.path.exists(args.mlp_pretrain) else None\n",
        "\n",
        "    # Initialize the NeuMF model\n",
        "    model = get_model(num_users, num_items, args.num_factors, eval(args.layers), eval(args.reg_layers), args.reg_mf)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=args.lr), loss='binary_crossentropy')\n",
        "\n",
        "    # Initial performance evaluation\n",
        "    print(\"Evaluating initial model...\")\n",
        "    hits, ndcgs = evaluate_model(model, testRatings, testNegatives, topK=10, evaluation_threads=1)\n",
        "    hr, ndcg = np.mean(hits), np.mean(ndcgs)\n",
        "    print(f'Init: HR = {hr:.4f}, NDCG = {ndcg:.4f}')\n",
        "    best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(args.epochs):\n",
        "        t1 = time.time()\n",
        "        user_input, item_input, labels = dataset.get_train_instances(train, args.num_neg)\n",
        "\n",
        "        hist = model.fit([np.array(user_input), np.array(item_input)], np.array(labels),\n",
        "                         batch_size=args.batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "        t2 = time.time()\n",
        "\n",
        "        # Evaluation\n",
        "        if epoch % args.verbose == 0:\n",
        "            hits, ndcgs = evaluate_model(model, testRatings, testNegatives, topK=10, evaluation_threads=1)\n",
        "            hr, ndcg = np.mean(hits), np.mean(ndcgs)\n",
        "            print(f'Epoch {epoch}: HR = {hr:.4f}, NDCG = {ndcg:.4f}, loss = {hist.history[\"loss\"][0]:.4f}')\n",
        "            if hr > best_hr:\n",
        "                best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
        "                if args.out:\n",
        "                    model.save(f'NeuMF_model_{epoch}.h5')\n",
        "\n",
        "    # Final output\n",
        "    print(f\"End. Best Iteration {best_iter}: HR = {best_hr:.4f}, NDCG = {best_ndcg:.4f}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: TypeError: evaluate_model() got an unexpected keyword argument 'topK'\n",
        "# correct me this code\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Input, Dense, Flatten, Concatenate, Multiply\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import time\n",
        "from evaluate import evaluate_model  # Ensure evaluate_model.py is uploaded in the Colab environment\n",
        "from Dataset import Dataset  # Ensure Dataset.py is uploaded in the Colab environment\n",
        "\n",
        "# Install required libraries if not already installed\n",
        "# !pip install tensorflow keras\n",
        "# !pip install h5py\n",
        "\n",
        "\n",
        "#################### Arguments ####################\n",
        "class Args:\n",
        "    path = './Data/'\n",
        "    dataset = 'ml-1m'\n",
        "    epochs = 100\n",
        "    batch_size = 256\n",
        "    num_factors = 8\n",
        "    layers = '[64,32,16,8]'\n",
        "    reg_mf = 0\n",
        "    reg_layers = '[0,0,0,0]'\n",
        "    num_neg = 4\n",
        "    lr = 0.001\n",
        "    verbose = 1\n",
        "    out = 1\n",
        "    mf_pretrain = 'ml-1m_GMF_model.h5'\n",
        "    mlp_pretrain = 'ml-1m_MLP_model.h5'\n",
        "\n",
        "args = Args()\n",
        "\n",
        "#################### Model Definition ####################\n",
        "def get_model(num_users, num_items, mf_dim=8, layers=[64, 32, 16, 8], reg_layers=[0, 0, 0, 0], reg_mf=0):\n",
        "    # Input layers\n",
        "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
        "    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
        "\n",
        "    # Embedding layers\n",
        "    MF_Embedding_User = Embedding(input_dim=num_users, output_dim=mf_dim, embeddings_regularizer=l2(reg_mf), input_length=1)\n",
        "    MF_Embedding_Item = Embedding(input_dim=num_items, output_dim=mf_dim, embeddings_regularizer=l2(reg_mf), input_length=1)\n",
        "\n",
        "    MLP_Embedding_User = Embedding(input_dim=num_users, output_dim=layers[0] // 2, embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
        "    MLP_Embedding_Item = Embedding(input_dim=num_items, output_dim=layers[0] // 2, embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
        "\n",
        "    # MF part\n",
        "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
        "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
        "    mf_vector = Multiply()([mf_user_latent, mf_item_latent])\n",
        "\n",
        "    # MLP part\n",
        "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
        "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
        "    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
        "    for idx in range(1, len(layers)):\n",
        "        mlp_vector = Dense(layers[idx], activation='relu', kernel_regularizer=l2(reg_layers[idx]))(mlp_vector)\n",
        "\n",
        "    # Concatenate MF and MLP parts\n",
        "    predict_vector = Concatenate()([mf_vector, mlp_vector])\n",
        "\n",
        "    # Prediction layer\n",
        "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name=\"prediction\")(predict_vector)\n",
        "\n",
        "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
        "    return model\n",
        "\n",
        "#################### Training & Evaluation ####################\n",
        "if __name__ == '__main__':\n",
        "    # Load Dataset\n",
        "    dataset = Dataset(args.path + args.dataset)\n",
        "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
        "    num_users, num_items = train.shape\n",
        "\n",
        "    # Load pre-trained models if available\n",
        "    gmf_model = tf.keras.models.load_model(args.mf_pretrain) if os.path.exists(args.mf_pretrain) else None\n",
        "    mlp_model = tf.keras.models.load_model(args.mlp_pretrain) if os.path.exists(args.mlp_pretrain) else None\n",
        "\n",
        "    # Initialize the NeuMF model\n",
        "    model = get_model(num_users, num_items, args.num_factors, eval(args.layers), eval(args.reg_layers), args.reg_mf)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=args.lr), loss='binary_crossentropy')\n",
        "\n",
        "    # Initial performance evaluation\n",
        "    print(\"Evaluating initial model...\")\n",
        "    hits, ndcgs = evaluate_model(model, testRatings, testNegatives, 10, 1) #Removed topK\n",
        "    hr, ndcg = np.mean(hits), np.mean(ndcgs)\n",
        "    print(f'Init: HR = {hr:.4f}, NDCG = {ndcg:.4f}')\n",
        "    best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(args.epochs):\n",
        "        t1 = time.time()\n",
        "        user_input, item_input, labels = dataset.get_train_instances(train, args.num_neg)\n",
        "\n",
        "        hist = model.fit([np.array(user_input), np.array(item_input)], np.array(labels),\n",
        "                         batch_size=args.batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "        t2 = time.time()\n",
        "\n",
        "        # Evaluation\n",
        "        if epoch % args.verbose == 0:\n",
        "            hits, ndcgs = evaluate_model(model, testRatings, testNegatives, 10, 1) #Removed topK\n",
        "            hr, ndcg = np.mean(hits), np.mean(ndcgs)\n",
        "            print(f'Epoch {epoch}: HR = {hr:.4f}, NDCG = {ndcg:.4f}, loss = {hist.history[\"loss\"][0]:.4f}')\n",
        "            if hr > best_hr:\n",
        "                best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
        "                if args.out:\n",
        "                    model.save(f'NeuMF_model_{epoch}.h5')\n",
        "\n",
        "    # Final output\n",
        "    print(f\"End. Best Iteration {best_iter}: HR = {best_hr:.4f}, NDCG = {best_ndcg:.4f}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "CitfRTKhv7KI",
        "outputId": "56b4a0a5-2632-4d96-b993-7cca1a4e4f32"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating initial model...\n",
            "Init: HR = 0.1070, NDCG = 0.0487\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Dataset' object has no attribute 'get_train_instances'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b3731a924ec4>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0muser_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         hist = model.fit([np.array(user_input), np.array(item_input)], np.array(labels),\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'get_train_instances'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NeuMF (with pre-training)_Output_Session1\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Input, Dense, Flatten, Concatenate, Multiply\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import time\n",
        "from evaluate import evaluate_model  # Ensure evaluate_model.py is uploaded in the Colab environment\n",
        "from Dataset import Dataset  # Ensure Dataset.py is uploaded in the Colab environment\n",
        "\n",
        "#################### Arguments ####################\n",
        "class Args:\n",
        "    path = './Data/'\n",
        "    dataset = 'ml-1m'\n",
        "    epochs = 20\n",
        "    batch_size = 256\n",
        "    num_factors = 8\n",
        "    layers = '[64,32,16,8]'\n",
        "    reg_mf = 0\n",
        "    reg_layers = '[0,0,0,0]'\n",
        "    num_neg = 4\n",
        "    lr = 0.001\n",
        "    verbose = 1\n",
        "    out = 1\n",
        "    mf_pretrain = 'ml-1m_GMF_model.h5'\n",
        "    mlp_pretrain = 'ml-1m_MLP_model.h5'\n",
        "\n",
        "args = Args()\n",
        "\n",
        "#################### Utility Function ####################\n",
        "def get_train_instances(train, num_negatives):\n",
        "    \"\"\"\n",
        "    Generate user, item, and label data for training.\n",
        "    :param train: The training matrix (users x items).\n",
        "    :param num_negatives: Number of negative samples per positive interaction.\n",
        "    :return: user_input, item_input, labels\n",
        "    \"\"\"\n",
        "    user_input, item_input, labels = [], [], []\n",
        "    num_users, num_items = train.shape\n",
        "    for (u, i) in train.keys():\n",
        "        # Positive instance\n",
        "        user_input.append(u)\n",
        "        item_input.append(i)\n",
        "        labels.append(1)\n",
        "        # Negative instances\n",
        "        for _ in range(num_negatives):\n",
        "            j = np.random.randint(num_items)\n",
        "            while (u, j) in train:\n",
        "                j = np.random.randint(num_items)\n",
        "            user_input.append(u)\n",
        "            item_input.append(j)\n",
        "            labels.append(0)\n",
        "    return user_input, item_input, labels\n",
        "\n",
        "#################### Model Definition ####################\n",
        "def get_model(num_users, num_items, mf_dim=8, layers=[64, 32, 16, 8], reg_layers=[0, 0, 0, 0], reg_mf=0):\n",
        "    # Input layers\n",
        "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
        "    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
        "\n",
        "    # Embedding layers\n",
        "    MF_Embedding_User = Embedding(input_dim=num_users, output_dim=mf_dim, embeddings_regularizer=l2(reg_mf), input_length=1)\n",
        "    MF_Embedding_Item = Embedding(input_dim=num_items, output_dim=mf_dim, embeddings_regularizer=l2(reg_mf), input_length=1)\n",
        "\n",
        "    MLP_Embedding_User = Embedding(input_dim=num_users, output_dim=layers[0] // 2, embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
        "    MLP_Embedding_Item = Embedding(input_dim=num_items, output_dim=layers[0] // 2, embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
        "\n",
        "    # MF part\n",
        "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
        "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
        "    mf_vector = Multiply()([mf_user_latent, mf_item_latent])\n",
        "\n",
        "    # MLP part\n",
        "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
        "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
        "    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
        "    for idx in range(1, len(layers)):\n",
        "        mlp_vector = Dense(layers[idx], activation='relu', kernel_regularizer=l2(reg_layers[idx]))(mlp_vector)\n",
        "\n",
        "    # Concatenate MF and MLP parts\n",
        "    predict_vector = Concatenate()([mf_vector, mlp_vector])\n",
        "\n",
        "    # Prediction layer\n",
        "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name=\"prediction\")(predict_vector)\n",
        "\n",
        "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
        "    return model\n",
        "\n",
        "#################### Training & Evaluation ####################\n",
        "if __name__ == '__main__':\n",
        "    # Load Dataset\n",
        "    dataset = Dataset(args.path + args.dataset)\n",
        "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
        "    num_users, num_items = train.shape\n",
        "\n",
        "    # Load pre-trained models if available\n",
        "    gmf_model = tf.keras.models.load_model(args.mf_pretrain) if os.path.exists(args.mf_pretrain) else None\n",
        "    mlp_model = tf.keras.models.load_model(args.mlp_pretrain) if os.path.exists(args.mlp_pretrain) else None\n",
        "\n",
        "    # Initialize the NeuMF model\n",
        "    model = get_model(num_users, num_items, args.num_factors, eval(args.layers), eval(args.reg_layers), args.reg_mf)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=args.lr), loss='binary_crossentropy')\n",
        "\n",
        "    # Initial performance evaluation\n",
        "    print(\"Evaluating initial model...\")\n",
        "    hits, ndcgs = evaluate_model(model, testRatings, testNegatives, 10, 1)  # Removed topK\n",
        "    hr, ndcg = np.mean(hits), np.mean(ndcgs)\n",
        "    print(f'Init: HR = {hr:.4f}, NDCG = {ndcg:.4f}')\n",
        "    best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(args.epochs):\n",
        "        t1 = time.time()\n",
        "        user_input, item_input, labels = get_train_instances(train, args.num_neg)\n",
        "\n",
        "        hist = model.fit([np.array(user_input), np.array(item_input)], np.array(labels),\n",
        "                         batch_size=args.batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "        t2 = time.time()\n",
        "\n",
        "        # Evaluation\n",
        "        if epoch % args.verbose == 0:\n",
        "            hits, ndcgs = evaluate_model(model, testRatings, testNegatives, 10, 1)  # Removed topK\n",
        "            hr, ndcg = np.mean(hits), np.mean(ndcgs)\n",
        "            print(f'Epoch {epoch}: HR = {hr:.4f}, NDCG = {ndcg:.4f}, loss = {hist.history[\"loss\"][0]:.4f}')\n",
        "            if hr > best_hr:\n",
        "                best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
        "                if args.out:\n",
        "                    model.save(f'NeuMF_model_{epoch}.h5')\n",
        "\n",
        "    # Final output\n",
        "    print(f\"End. Best Iteration {best_iter}: HR = {best_hr:.4f}, NDCG = {best_ndcg:.4f}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "wfpX9MStzjH-",
        "outputId": "6f1b8769-3bf1-4cdb-9a9f-aa492fb4563e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-dd5c0e8da263>\u001b[0m in \u001b[0;36m<cell line: 91>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# Load Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestRatings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestNegatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestRatings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestNegatives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mConstructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         '''\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_rating_file_as_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".train.rating\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestRatings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_rating_file_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".test.rating\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtestNegatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_negative_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".test.negative\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Dataset.py\u001b[0m in \u001b[0;36mload_rating_file_as_matrix\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mnum_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mnum_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Construct matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NeuMF (without pre-training)_Output_Session1\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Embedding, Input, Dense, Flatten, Concatenate, Multiply\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "import os\n",
        "import time\n",
        "from evaluate import evaluate_model  # Ensure evaluate_model.py is uploaded in the Colab environment\n",
        "from Dataset import Dataset  # Ensure Dataset.py is uploaded in the Colab environment\n",
        "\n",
        "#################### Arguments ####################\n",
        "class Args:\n",
        "    path = './Data/'\n",
        "    dataset = 'ml-1m'\n",
        "    epochs = 100\n",
        "    batch_size = 256\n",
        "    num_factors = 8\n",
        "    layers = '[64,32,16,8]'\n",
        "    reg_mf = 0\n",
        "    reg_layers = '[0,0,0,0]'\n",
        "    num_neg = 4\n",
        "    lr = 0.001\n",
        "    verbose = 1\n",
        "    out = 1\n",
        "    # Removed pre-trained model paths since we are not using pre-training\n",
        "    mf_pretrain = None\n",
        "    mlp_pretrain = None\n",
        "\n",
        "args = Args()\n",
        "\n",
        "#################### Utility Function ####################\n",
        "def get_train_instances(train, num_negatives):\n",
        "    \"\"\"\n",
        "    Generate user, item, and label data for training.\n",
        "    :param train: The training matrix (users x items).\n",
        "    :param num_negatives: Number of negative samples per positive interaction.\n",
        "    :return: user_input, item_input, labels\n",
        "    \"\"\"\n",
        "    user_input, item_input, labels = [], [], []\n",
        "    num_users, num_items = train.shape\n",
        "    for (u, i) in train.keys():\n",
        "        # Positive instance\n",
        "        user_input.append(u)\n",
        "        item_input.append(i)\n",
        "        labels.append(1)\n",
        "        # Negative instances\n",
        "        for _ in range(num_negatives):\n",
        "            j = np.random.randint(num_items)\n",
        "            while (u, j) in train:\n",
        "                j = np.random.randint(num_items)\n",
        "            user_input.append(u)\n",
        "            item_input.append(j)\n",
        "            labels.append(0)\n",
        "    return user_input, item_input, labels\n",
        "\n",
        "#################### Model Definition ####################\n",
        "def get_model(num_users, num_items, mf_dim=8, layers=[64, 32, 16, 8], reg_layers=[0, 0, 0, 0], reg_mf=0):\n",
        "    # Input layers\n",
        "    user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
        "    item_input = Input(shape=(1,), dtype='int32', name='item_input')\n",
        "\n",
        "    # Embedding layers\n",
        "    MF_Embedding_User = Embedding(input_dim=num_users, output_dim=mf_dim, embeddings_regularizer=l2(reg_mf), input_length=1)\n",
        "    MF_Embedding_Item = Embedding(input_dim=num_items, output_dim=mf_dim, embeddings_regularizer=l2(reg_mf), input_length=1)\n",
        "\n",
        "    MLP_Embedding_User = Embedding(input_dim=num_users, output_dim=layers[0] // 2, embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
        "    MLP_Embedding_Item = Embedding(input_dim=num_items, output_dim=layers[0] // 2, embeddings_regularizer=l2(reg_layers[0]), input_length=1)\n",
        "\n",
        "    # MF part\n",
        "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
        "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
        "    mf_vector = Multiply()([mf_user_latent, mf_item_latent])\n",
        "\n",
        "    # MLP part\n",
        "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
        "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
        "    mlp_vector = Concatenate()([mlp_user_latent, mlp_item_latent])\n",
        "    for idx in range(1, len(layers)):\n",
        "        mlp_vector = Dense(layers[idx], activation='relu', kernel_regularizer=l2(reg_layers[idx]))(mlp_vector)\n",
        "\n",
        "    # Concatenate MF and MLP parts\n",
        "    predict_vector = Concatenate()([mf_vector, mlp_vector])\n",
        "\n",
        "    # Prediction layer\n",
        "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name=\"prediction\")(predict_vector)\n",
        "\n",
        "    model = Model(inputs=[user_input, item_input], outputs=prediction)\n",
        "    return model\n",
        "\n",
        "#################### Training & Evaluation ####################\n",
        "if __name__ == '__main__':\n",
        "    # Load Dataset\n",
        "    dataset = Dataset(args.path + args.dataset)\n",
        "    train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
        "    num_users, num_items = train.shape\n",
        "\n",
        "    # Initialize the NeuMF model (without pre-training)\n",
        "    model = get_model(num_users, num_items, args.num_factors, eval(args.layers), eval(args.reg_layers), args.reg_mf)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=Adam(learning_rate=args.lr), loss='binary_crossentropy')\n",
        "\n",
        "    # Initial performance evaluation\n",
        "    print(\"Evaluating initial model...\")\n",
        "    hits, ndcgs = evaluate_model(model, testRatings, testNegatives, 10, 1)  # Removed topK\n",
        "    hr, ndcg = np.mean(hits), np.mean(ndcgs)\n",
        "    print(f'Init: HR = {hr:.4f}, NDCG = {ndcg:.4f}')\n",
        "    best_hr, best_ndcg, best_iter = hr, ndcg, -1\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(args.epochs):\n",
        "        t1 = time.time()\n",
        "        user_input, item_input, labels = get_train_instances(train, args.num_neg)\n",
        "\n",
        "        hist = model.fit([np.array(user_input), np.array(item_input)], np.array(labels),\n",
        "                         batch_size=args.batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "        t2 = time.time()\n",
        "\n",
        "        # Evaluation\n",
        "        if epoch % args.verbose == 0:\n",
        "            hits, ndcgs = evaluate_model(model, testRatings, testNegatives, 10, 1)  # Removed topK\n",
        "            hr, ndcg = np.mean(hits), np.mean(ndcgs)\n",
        "            print(f'Epoch {epoch}: HR = {hr:.4f}, NDCG = {ndcg:.4f}, loss = {hist.history[\"loss\"][0]:.4f}')\n",
        "            if hr > best_hr:\n",
        "                best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
        "                if args.out:\n",
        "                    model.save(f'NeuMF_model_{epoch}.h5')\n",
        "\n",
        "    # Final output\n",
        "    print(f\"End. Best Iteration {best_iter}: HR = {best_hr:.4f}, NDCG = {best_ndcg:.4f}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "0iEMn9dIGzxh",
        "outputId": "c88f41cb-852b-46ed-8d37-a18c3e820eb6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating initial model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-a56502eb2498>\u001b[0m in \u001b[0;36m<cell line: 93>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# Initial performance evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating initial model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mhits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestRatings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestNegatives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Removed topK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mhr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndcg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndcgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Init: HR = {hr:.4f}, NDCG = {ndcg:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/evaluate.py\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, testRatings, testNegatives, K, num_thread)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Single thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_testRatings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mndcg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_one_rating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mhits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mndcgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndcg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/evaluate.py\u001b[0m in \u001b[0;36meval_one_rating\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mmap_item_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     predictions = _model.predict([users, np.array(items)], \n\u001b[0m\u001b[1;32m     63\u001b[0m                                  batch_size=100, verbose=0)\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    448\u001b[0m     ):\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# Create an iterator that yields batches of input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         epoch_iterator = TFEpochIterator(\n\u001b[0m\u001b[1;32m    451\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribute_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrab_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             dataset = dataset.map(\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0mgrab_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2309\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2312\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     38\u001b[0m         input_dataset, map_func, preserve_cardinality=True, name=name)\n\u001b[1;32m     39\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m    985\u001b[0m     \u001b[0mdeps_control_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNullContextmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps_control_manager\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdeps_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m     \u001b[0mcurrent_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m     \u001b[0mdefault_use_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mas_default\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtf_contextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0minner_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m       \u001b[0;34m\"\"\"Context manager for copying distribute.Strategy scope information.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_contextlib.py\u001b[0m in \u001b[0;36mcontextmanager\u001b[0;34m(target)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \"\"\"\n\u001b[1;32m     38\u001b[0m   \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_contextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'contextmanager'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36mmake_decorator\u001b[0;34m(target, decorator_func, decorator_name, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdecorator_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mdecorator_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m   decorator = TFDecorator(decorator_name, target, decorator_doc,\n\u001b[0m\u001b[1;32m    137\u001b[0m                           decorator_argspec)\n\u001b[1;32m    138\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecorator_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tf_decorator'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, decorator_name, target, decorator_doc, decorator_argspec)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__signature__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;31m# Certain callables such as builtins can not be inspected for signature.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   3250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3252\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3253\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3254\u001b[0m     return Signature.from_callable(obj, follow_wrapped=follow_wrapped,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}